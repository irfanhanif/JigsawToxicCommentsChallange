{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Toxic Comments Classification Challange</h1><br>\n",
    "Implementasi menggunakan <strong>Word2Vec</strong> dan <strong>Convolutional Neural Networks</strong><br>\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import mysql.connector\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "from tflearn.data_utils import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut dilakukan pendefinisian path untuk file-file yang digunakan\n",
    "\n",
    "- Corpus yang digunakan adalah 100milyar kata yang diambil dari Google News. Pada corpus ini dijalankan algoritma Word2Vec yang menghasilkan word embeddings. Setiap kata terdiri dari vektor berdimensi 300. File bisa di-download di https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "corpus = 'GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "TRAIN_DATA_FILE = data_path + 'train.csv'\n",
    "TEST_DATA_FILE = data_path + 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan Preprocessing pasa setiap teks (DONE - comments_np.npy)\n",
    "1. Melakukan tokenisasi, memisahkan setiap kata\n",
    "2. Membuang kata-kata yang termasuk dalam stopwords\n",
    "3. Membuang string yang mengandung karakter selain huruf dan tanda petik satu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:8: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def preprocess(string):\n",
    "    raw = string.split(' ')\n",
    "    remover = re.compile(\"[^a-zA-Z']\")\n",
    "    \n",
    "    token = []\n",
    "    \n",
    "    for i in raw:\n",
    "        if i in stopwords.words('english'):\n",
    "            continue\n",
    "        term = remover.sub('', i).lower()\n",
    "        token.append(term)\n",
    "    \n",
    "    row = filter(None, token)\n",
    "    \n",
    "    return np.array(row)\n",
    "\n",
    "comments_text = train['comment_text'].values\n",
    "comments = []\n",
    "for comm in comments_text:\n",
    "    tmp = comm.replace('\\n', ' ')\n",
    "    tmp = comm.replace('.', '. ')\n",
    "    tmp = preprocess(comm)\n",
    "    \n",
    "    comments.append(tmp)\n",
    "    \n",
    "comments_np = np.array(comments)\n",
    "np.save('comments_np.npy', comments_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan proses untuk mendapatkan <strong>HOT-VECTOR</strong> pada setiap kelas sentiment<br>\n",
    "Kelas-kelasnya adalah:\n",
    "1. toxic\n",
    "2. severe_toxic\n",
    "3. obscene\n",
    "4. threat\n",
    "5. insult\n",
    "6. identity_hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_cls = train[\"toxic\"].values\n",
    "severe_toxic_cls = train[\"severe_toxic\"].values\n",
    "obscene_cls = train[\"obscene\"].values\n",
    "threat_cls = train[\"threat\"].values\n",
    "insult_cls = train[\"insult\"].values\n",
    "identity_hate_cls = train[\"identity_hate\"].values\n",
    "\n",
    "comments_cls_list = []\n",
    "for i in range(len(comments_np)):\n",
    "    hot_vec_list = []\n",
    "    \n",
    "    hot_vec_list.append(toxic_cls[i])\n",
    "    hot_vec_list.append(severe_toxic_cls[i])\n",
    "    hot_vec_list.append(obscene_cls[i])\n",
    "    hot_vec_list.append(threat_cls[i])\n",
    "    hot_vec_list.append(insult_cls[i])\n",
    "    hot_vec_list.append(identity_hate_cls[i])\n",
    "    \n",
    "    hot_vec_np = np.array(hot_vec_list)\n",
    "    comments_cls_list.append(hot_vec_np)\n",
    "    \n",
    "comments_cls_np = np.array(comments_cls_list)\n",
    "np.save(\"comments_cls_np.npy\", comments_cls_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FROM CHECKPOINTS\n",
    "comments_np = np.load(\"comments_np.npy\")\n",
    "# comments_cls_np = (\"comments_cls_np.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mencari comment dengan kata terpanjang...\n",
      "Jumlah kata pada comment terpanjang = 1380\n"
     ]
    }
   ],
   "source": [
    "print \"Mencari comment dengan kata terpanjang...\"\n",
    "max_comment_len = -1\n",
    "for c in comments_np:\n",
    "    if len(c) > max_comment_len:\n",
    "        max_comment_len = len(c)\n",
    "print \"Jumlah kata pada comment terpanjang = \" + str(max_comment_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "max_feature = embedding_size * max_comment_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format(corpus, binary=True)\n",
    "word_vectors.save_word2vec_format('w2v.txt', binary=False)\n",
    "\n",
    "db = mysql.connector.connect(user=\"root\", database=\"wor2vec\")\n",
    "cursor = db.cursor()\n",
    "\n",
    "pbar = tqdm(total=3000000)\n",
    "with open(\"w2v.txt\") as f:\n",
    "    try:\n",
    "        for line in f:\n",
    "            wv = line.split('\\n')\n",
    "            wv = wv[0].split()\n",
    "            key = wv[0]\n",
    "            vec = wv[1:]\n",
    "            json_vec = json.JSONEncoder().encode(vec)\n",
    "            #decoded_vec = json.JSONDecoder().decode(json_vec)\n",
    "            #vec_d = np.asarray(decoded_vec, dtype=np.float32)\n",
    "\n",
    "            sql = \"insert into word (word, vec) values('\"+ key +\"', '\" + json_vec + \"')\"\n",
    "            cursor.execute(sql)\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "        print \"DB COMMIT...\"\n",
    "        db.commit()\n",
    "    except mysql.connector.Error as err:\n",
    "        print(\"Something went wrong: {}\".format(err))\n",
    "        print sql\n",
    "        db.rollback()\n",
    "\n",
    "print \"SUCCESS\"\n",
    "db.close()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = mysql.connector.connect(user=\"root\", database=\"wor2vec\")\n",
    "cursor = db.cursor(buffered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 2114/95851 [03:44<3:05:30,  8.42it/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-0f6698b473f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mbegin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcmt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetWordEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-0f6698b473f9>\u001b[0m in \u001b[0;36mgetWordEmbedding\u001b[0;34m(word, cursor)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#     word = word.replace(\"'\", \"''\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\"select vec from term where term like %s\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mysql/connector/cursor.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, operation, params, multi)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmd_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstmt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterfaceError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_have_next_result\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=W0212\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mysql/connector/connection.pyc\u001b[0m in \u001b[0;36mcmd_query\u001b[0;34m(self, query, raw, buffered, raw_as_string)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_cmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mServerCmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUERY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_have_next_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mysql/connector/connection.pyc\u001b[0m in \u001b[0;36m_send_cmd\u001b[0;34m(self, command, argument, packet_number, packet, expect_response)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexpect_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msend_empty_packet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mysql/connector/network.pyc\u001b[0m in \u001b[0;36mrecv_plain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mpacket_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mpacket_len\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                 \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpacket_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterfaceError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2013\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def getWordEmbedding(word, cursor):\n",
    "#     word = word.replace(\"'\", \"''\")\n",
    "    sql = \"\"\"select vec from term where term like %s\"\"\"\n",
    "    cursor.execute(sql, (str(word),))\n",
    "    data = cursor.fetchall()\n",
    "    if len(data) > 0:\n",
    "        decoded_vec = json.JSONDecoder().decode(data[0][0])\n",
    "        vec = np.asarray(decoded_vec, dtype=np.float32)\n",
    "        return True, vec\n",
    "    else:\n",
    "        return False, data\n",
    "\n",
    "def padSequence(feature, max_feature):\n",
    "    pad = np.zeros(max_feature-len(feature), dtype=np.float32)\n",
    "    feature = np.concatenate([feature, pad])\n",
    "    return feature\n",
    "\n",
    "ftr_list = []\n",
    "pbar = tqdm(total=len(comments_np), file=sys.stdout)\n",
    "for cmt in comments_np:\n",
    "    begin = True\n",
    "    for word in cmt:\n",
    "        stat, vec = getWordEmbedding(word, cursor)\n",
    "        if not stat:\n",
    "            continue\n",
    "        if begin:\n",
    "            begin = False\n",
    "            feature = vec\n",
    "        else:\n",
    "            feature = np.concatenate([feature, vec])\n",
    "    ftr_list.append(padSequence(feature, max_feature))\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "ftr_np = np.array(ftr_list)\n",
    "np.save(\"ftr_np.npy\", ftr_np)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
